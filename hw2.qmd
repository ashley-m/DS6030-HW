---
title: "Homework #2: Resampling" 
author: "**Ashley Miller**"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```


# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
data_dir = 'https://mdporter.github.io/teaching/data/' # data directory
library(tidymodels)# for optional tidymodels solutions
library(tidyverse) # functions for data manipulation  
```


# Problem 1: Bootstrapping 

Bootstrap resampling can be used to quantify the uncertainty in a fitted curve. 

## a. Data Generating Process

Create a set of functions to generate data from the following distributions:
\begin{align*}
X &\sim \mathcal{U}(0, 2) \qquad \text{Uniform between $0$ and $2$}\\
Y &= 1 + 2x + 5\sin(5x) + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma=2.5)
\end{align*}

::: {.callout-note title="Solution"}

```{r}
sim_x <- function(n) runif(n, 0, 2)
f <- function(x) 1+2*x + 5*sin(5*x)
sim_y <- function(x, sd = 2.5) {
  n=length(x)
  f(x) + rnorm(n, sd = sd)
}
```

:::

## b. Simulate data

Simulate $n=100$ realizations from these distributions. Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$. Use `set.seed(211)` prior to generating the data.

::: {.callout-note title="Solution"}

```{r}
set.seed(211)
n = 100
X = sim_x(n)
Y = sim_y(X)
data_train = tibble(X, Y)
library(ggplot2)

data_train %>%
  ggplot(aes(X, Y)) + geom_point() + geom_function(fun=f, color = "blue")
```

:::


## c. 5th degree polynomial fit

Fit a 5th degree polynomial. Produce a scatterplot and draw the *estimated* regression curve.

::: {.callout-note title="Solution"}

```{r}
quint_mod <- lm(Y~poly(X, degree = 5), data = data_train)
true_data <- data.frame(X = seq(min(data_train$X), max(data_train$X), length.out = 100))
true_data$Y <- f(true_data$X)

yhat = predict(quint_mod, tibble(X=data_train$X))

est_data <- data.frame(X= data_train$X, Y = yhat)

data_train %>%
  ggplot(aes(X, Y)) + geom_point() + 
  geom_line(data = true_data, aes(X, Y, color = "True"), linewidth = 1) + 
  geom_line(data = est_data, aes(X, Y, color = "Est"), linewidth = 1) +
  scale_color_manual(
    name = 'Regression Type',
    breaks = c("Est", "True"),
    values = c( "Est" = "red", "True" ="blue")
    )
```

:::


## d. Bootstrap sampling

Make 200 bootstrap samples. For each bootstrap sample, fit a 5th degree polynomial and make predictions at `eval_pts = seq(0, 2, length=100)`

- Set the seed (use `set.seed(212)`) so your results are reproducible.
- Produce a scatterplot with the original data and add the 200 bootstrap curves

::: {.callout-note title="Solution"}

```{r}
set.seed(212)
eval_pts = seq(0,2,length = 100)
M = 200
data_eval = tibble(X=eval_pts)
YHAT = matrix(NA, nrow(data_eval), M)
for(m in 1:M){
  ind = sample(n, replace = T)
  m_boot = lm(Y~poly(X, degree = 5), data= data_train[ind,])
  YHAT[,m] = predict(m_boot, data_eval)
}
data_fitted = as_tibble(YHAT) %>%
  bind_cols(data_eval) %>%
  pivot_longer(-X ,names_to = "simulation", values_to = "Y")
ggplot(data_train, aes(X, Y)) +
  geom_line(data = data_fitted, color = "red", alpha = 0.1, aes(group = simulation)) +
  geom_point()
```

:::
    
## e. Confidence Intervals

Calculate the pointwise 95% confidence intervals from the bootstrap samples. That is, for each $x \in {\rm eval\_pts}$, calculate the upper and lower limits such that only 5% of the curves fall outside the interval at $x$. 

- Remake the plot from part *c*, but add the upper and lower boundaries from the 95% confidence intervals. 

::: {.callout-note title="Solution"}

```{r}
conf_intervals = apply(YHAT, 1, function(y) {
  quantile(y, probs = c(0.025, 0.975))
})

CI = tibble(
  X = eval_pts,
  lower = conf_intervals[1, ],
  upper = conf_intervals[2, ]
)
data_train %>%
  ggplot(aes(X, Y)) + geom_point() + 
  geom_line(data = true_data, aes(X, Y, color = "True"), linewidth = 1) +
  geom_ribbon(data = CI, aes(x = X, ymin = lower, ymax = upper), alpha = 0.2, fill = "purple") +
    geom_line(data = est_data, aes(X, Y, color = "Est"), linewidth = 1) +
  scale_color_manual(
    name = 'Regression Type',
    breaks = c("Est", "True"),
    values = c( "Est" = "red", "True" ="blue")
    )
```

:::

# Problem 2: V-Fold cross-validation with $k$ nearest neighbors

Run 10-fold cross-validation on the data generated in part 1b to select the optimal $k$ in a k-nearest neighbor (kNN) model. Then evaluate how well cross-validation performed by evaluating the performance on a large test set. The steps below will guide you.


## a. Implement 10-fold cross-validation

Use $10$-fold cross-validation to find the value of $k$ (i.e., neighborhood size) that provides the smallest cross-validated MSE using a kNN model. 

- Search over $k=3,4,\ldots, 40$.
- Use `set.seed(221)` prior to generating the folds to ensure the results are replicable. 
- Show the following:
    - the optimal $k$ (as determined by cross-validation)
    - the corresponding estimated MSE
    - produce a plot with $k$ on the x-axis and the estimated MSE on the y-axis (optional: add 1-standard error bars). 
- Notation: The $k$ is the tuning paramter for the kNN model. The $v=10$ is the number of folds in V-fold cross-validation. Don't get yourself confused.

::: {.callout-note title="Solution"}

```{r}
library(FNN)
set.seed(221)
folds = 10

knn_eval <- function(data_fit, data_eval, df = seq(3, 40, by=1)) {
MSE = numeric(length(df)) # initialize
MSE_tst = numeric(length(df))
for(i in 1:length(df)) {
# set tuning parameter value
df_i = df[i]
# fit with training data
fit = knn.reg(data_fit[,'X', drop = F],
              y = data_fit$Y,
              test= data_eval[,"X", drop=F],
              k = df_i)
# get errors / loss
MSE[i] = mean( (data_eval$Y-fit$pred)**2 )

}
tibble(df = df, mse=MSE, edf = nrow(data_fit)/df) # output
}

fold = sample(rep(1:folds, length=n))
results = vector("list", folds)
#- Iterate over folds
for(j in 1:folds){
  #-- Set training/val data
  val = which(fold == j) # indices of holdout/validation data
  train = which(fold != j) # indices of fitting/training data
  n.val = length(val) # number of observations in validation
  #- fit and evaluate models
  results[[j]] = knn_eval(
        data_fit = slice(data_train, train),
        data_eval = slice(data_train, val)
        ) %>%
    mutate(fold = j, n.val) # add fold number and number in validation
}
RESULTS = bind_rows(results)

sum_results <- RESULTS %>% 
  group_by(df) %>%
  summarize(
    mean_mse = mean(mse),
    mse_se = sd(mse)/sqrt(nrow(data_train))
  )

RESULTS <- RESULTS %>%
  left_join(sum_results, by = "df")


RESULTS %>% mutate(fold = factor(fold)) %>%
  group_by(df) %>%
    ggplot(aes(df, mse)) +
    geom_line(data = . %>% group_by(df) %>% summarize(mse = mean(mse)), linewidth=1) +
    geom_point(data = . %>% group_by(df) %>% summarize(mse = mean(mse)) %>%
    slice_min(mse, n=1), size=3, color="red") +
    geom_errorbar(aes(x=df, ymax=mean_mse+mse_se, ymin=mean_mse-mse_se)) +
    scale_x_continuous(breaks = seq(0, 40, by=1))

new <- RESULTS %>%
  group_by(df) %>%
  summarize(mse = mean(mse))

min(new$mse)

```

:::


## b. Find the optimal *edf*

The $k$ (number of neighbors) in a kNN model determines the effective degrees of freedom *edf*. What is the optimal *edf*? Be sure to use the correct sample size when making this calculation. Produce a plot similar to that from part *a*, but use *edf* (effective degrees of freedom) on the x-axis. 

::: {.callout-note title="Solution"}

```{r}

sum_results <- RESULTS %>% 
  group_by(edf) %>%
  summarize(
    mean_mse = mean(mse),
    mse_se = sd(mse)/sqrt(nrow(data_train))
  )

RESULTS <- RESULTS %>%
  left_join(sum_results, by = "edf")

RESULTS %>% mutate(fold = factor(fold)) %>%
  group_by(edf) %>%
    ggplot(aes(edf, mse)) +
    geom_line(data = . %>% group_by(edf) %>% summarize(mse = mean(mse)), linewidth=1) +
    geom_point(data = . %>% group_by(edf) %>% summarize(mse = mean(mse)) %>%
    slice_min(mse, n=1), size=3, color="red") +
    geom_errorbar(aes(x=edf, ymax=mean_mse.x+mse_se.x, ymin=mean_mse.x-mse_se.x)) +
    scale_x_continuous(breaks = seq(0, 40, by=1))

new2 <- RESULTS %>%
  group_by(edf, df) %>%
  summarize(mse = mean(mse))

# optimal k value
new2$df[which.min(new2$mse)]

# edf of said model
new2$edf[which.min(new2$mse)]

# mse of optimal KNN model
min(new2$mse)
```

:::

## c. Choose $k$

After running cross-validation, a final model fit from *all* of the training data needs to be produced to make predictions. What value of $k$ would you choose? Why? 

::: {.callout-note title="Solution"}

I would choose k = 8 because the effective degrees of freedom is equal to the number of rows in the training set/k and the minimum point seen in both graphs correspond to this value.

:::

## d. Evaluate actual performance

Now we will see how well cross-validation performed. Simulate a test data set of $50000$ observations from the same distributions. Use `set.seed(223)` prior to generating the test data. 

- Fit a set of kNN models, using the full training data, and calculate the mean squared error (MSE) on the test data for each model. Use the same $k$ values in *a*. 
- Report the optimal $k$, the corresponding *edf*, and MSE based on the test set.

::: {.callout-note title="Solution"}

```{r}
set.seed(223)
n = 50000
x_t = sim_x(n)
y_t = sim_y(x_t)
data_test <- data.frame(X = x_t, Y = y_t)

new3<- knn_eval(data_train, data_test)

new3 %>%
  group_by(df) %>%
  summarize(mse = mean(mse))%>%
  ggplot(aes(df, mse)) +
    geom_line(data = . %>% group_by(df) %>% summarize(mse = mean(mse)), linewidth=1) +
    geom_point(data = . %>% group_by(df) %>% summarize(mse = mean(mse)) %>%
    slice_min(mse, n=1), size=3, color="red") +
    scale_x_continuous(breaks = seq(0, 40, by=1))

# optimal k value
new3$df[which.min(new3$mse)]

# edf of said model
new3$edf[which.min(new3$mse)]

# mse of optimal KNN model
min(new3$mse)

```

:::

## e. Performance plots

Plot both the cross-validation estimated and (true) error calculated from the test data on the same plot. See Figure 5.6 in ISL (pg 182) as a guide. 

- Produce two plots: one with $k$ on the x-axis and one with *edf* on the x-axis.
- Each plot should have two lines: one from part *a* and one from part *d* 
    
::: {.callout-note title="Solution"}
```{r}

new3_summary <- new3 %>%
  group_by(df) %>%
  summarize(mse = mean(mse)) %>%
  mutate(group = "Group 1")

new2_summary <- new2 %>%
  group_by(df) %>%
  summarize(mse = mean(mse)) %>%
  mutate(group = "Group 2")

combined_data <- bind_rows(new3_summary, new2_summary)

# Plot with ggplot2
combined_data %>%
  ggplot(aes(df, mse, color = group)) +
  geom_line(data = new3_summary, aes(color = "Group 1"), linewidth = 1) +
  geom_line(data = new2_summary, aes(color = "Group 2"), linewidth = 1) +
  geom_point(data = new3_summary %>% slice_min(mse, n = 1), size = 3, aes(color = "Group 1")) +
  geom_point(data = new2_summary %>% slice_min(mse, n = 1), size = 3, aes(color = "Group 2")) +
  scale_color_manual(values = c("Group 1" = "red", "Group 2" = "blue"), 
                     labels = c("Group 1" = "Test Group", "Group 2" = "Train Group")) +
  scale_x_continuous(breaks = seq(0, 40, by = 1)) +
  labs(color = "Legend")
```
```{r}
new3_summary <- new3 %>%
  group_by(edf) %>%
  summarize(mse = mean(mse)) %>%
  mutate(group = "Group 1")

new2_summary <- new2 %>%
  group_by(edf) %>%
  summarize(mse = mean(mse)) %>%
  mutate(group = "Group 2")

combined_data <- bind_rows(new3_summary, new2_summary)

combined_data %>%
  ggplot(aes(edf, mse, color = group)) +
  geom_line(data = new3_summary, aes(color = "Group 1"), linewidth = 1) +
  geom_line(data = new2_summary, aes(color = "Group 2"), linewidth = 1) +
  geom_point(data = new3_summary %>% slice_min(mse, n = 1), size = 3, aes(color = "Group 1")) +
  geom_point(data = new2_summary %>% slice_min(mse, n = 1), size = 3, aes(color = "Group 2")) +
  scale_color_manual(values = c("Group 1" = "red", "Group 2" = "blue"), 
                     labels = c("Group 1" = "Test Group", "Group 2" = "Train Group")) +
  scale_x_continuous(breaks = seq(0, 40, by = 1)) +
  labs(color = "Legend")
```

:::
    
## f. Did cross-validation work as intended?

Based on the plots from *e*, does it appear that cross-validation worked as intended? How sensitive is the choice of $k$ on the resulting test MSE?      

::: {.callout-note title="Solution"}

The plots are fairly flat at the portion chosen by cross-validation on both the train and test groups. This means that the MSE is not very sensitive to variations in k below the minimum MSE in the test group with much more data. The effective degrees of freedom remains flatter on the right side of the minimum MSE, which makes sense because it is inversely related to k. If one chooses to add standard error bars you could even more handily choose the least complex model that is within 1 standard error of the minimum. Extremely low values of k, however, vary much more greatly as each individual point would have more influence on the model.

:::




