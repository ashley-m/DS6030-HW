---
title: "Final Exam" 
author: "**Ashley Miller**"
format: ds6030hw-html
cache: true
---

::: {style="background-color:yellow; color:red; display: block; border-color: black; padding:1em; padding-bottom: .5em;"}
This is an **independent assignment**. Do not discuss or work with classmates.
:::

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set(warning = FALSE, message = FALSE)                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Grading Notes

- The exam is graded out of 100 pts.
- 20 points are given for overall style and ease of reading. If you don't use the homework format or print out pages of unnecessary output the style points will be reduced. 
- The point totals for each question are provided below.
- Be sure to show your work so you can get partial credit even if your solution is wrong. 

# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
data_dir = 'https://mdporter.github.io/teaching/data' # data directory
library(tidyverse) # functions for data manipulation   
library(mclust)    # model based clustering
library(mixtools)  # for poisson mixture models
library(dendextend)
library(tidymodels)
library(yardstick)
```


# Problem 1: Customer Segmentation (15 pts)

RFM analysis is an approach that some businesses use to understand their customers' activities. At any point in time, a company can measure how recently a customer purchased a product (Recency), how many times they purchased a product (Frequency), and how much they have spent (Monetary Value). There are many ad-hoc attempts to segment/cluster customers based on the RFM scores (e.g., here is one based on using the customers' rank of each dimension independently: <https://joaocorreia.io/blog/rfm-analysis-increase-sales-by-segmenting-your-customers.html>). In this problem you will use the clustering methods we covered in class to segment the customers. 

The data for this problem can be found here: <`r file.path(data_dir, "RFM.csv")`>. Cluster based on the `Recency`, `Frequency`, and `Monetary` features.

## a. Load the data (3 pts)

::: {.callout-note title="Solution"}
```{r}
rfm <- read.csv("RFM.csv")

```
:::


## b. Implement hierarchical clustering. (3 pts)

- Describe any pre-processing steps you took (e.g., scaling, distance metric)
- State the linkage method you used with justification. 
- Show the resulting dendrogram.
- State the number of segments/clusters you used with justification. 
- Using your segmentation, are customers 1 and 12 in the same cluster?     

::: {.callout-note title="Solution"}
```{r}
# Standardize by transforming to Z-score
rfm_std <- rfm %>%
  mutate(
    Recency = (Recency - mean(Recency))/sd(Recency),
    Frequency = (Frequency - mean(Frequency))/sd(Frequency),
    Monetary = (Monetary - mean(Monetary))/sd(Monetary)
  )

# View distributions 
ggplot(rfm_std, aes(x = Recency)) +
  geom_histogram(binwidth = 0.25, fill = "blue", alpha = 0.7) +
  labs(title = "Distribution of Recency")

ggplot(rfm_std, aes(x = Frequency)) +
  geom_histogram(binwidth = 0.25, fill = "green", alpha = 0.7) +
  labs(title = "Distribution of Frequency")

ggplot(rfm_std, aes(x = Monetary)) +
  geom_histogram(binwidth = 0.25, fill = "red", alpha = 0.7) +
  labs(title = "Distribution of Monetary")
```
```{r}
data <- rfm_std[, -1]
dist_m <- dist(data)

# Example with complete linkage
hc_complete <- hclust(dist_m, method = "complete")

# Example with single linkage
hc_single <- hclust(dist_m, method = "single")

# Example with average linkage
hc_average <- hclust(dist_m, method = "average")

# Example with Ward's method
hc_ward <- hclust(dist_m, method = "ward.D2")

```
```{r}
# Create the tibble with heights and cluster numbers
elbow_data <- tibble(height = hc_ward$height, K = row_number(-height))

# Calculate differences in height between consecutive points
elbow_data <- elbow_data %>%
  mutate(height_diff = c(NA, diff(height)))

# Normalize the height differences
elbow_data <- elbow_data %>%
  mutate(height_diff_norm = (height_diff - mean(height_diff, na.rm = TRUE)) / sd(height_diff, na.rm = TRUE))

# Calculate second derivative to find the elbow point and bind the result
second_derivative <- c(NA, diff(elbow_data$height_diff_norm, differences = 2), NA)

# Use absolute value of the second derivative
second_derivative_abs <- abs(second_derivative)

# Bind the second derivative to the elbow_data
elbow_data <- elbow_data %>%
  mutate(second_derivative = second_derivative,
         second_derivative_abs = second_derivative_abs)

# Find the changepoint by identifying the maximum absolute second derivative
changepoint_k <- elbow_data$K[[which.max(elbow_data$second_derivative_abs)]]

tibble(height = hc_ward$height, K = row_number(-height)) %>%
ggplot(aes(K, height)) +
geom_line() +
geom_point(aes(color = ifelse(K == changepoint_k, "red", "black"))) +
scale_color_identity() +
coord_cartesian(xlim=c(1, 30))
```
```{r}
# Create dendrograms
dend_complete <- as.dendrogram(hc_complete)
dend_single <- as.dendrogram(hc_single)
dend_average <- as.dendrogram(hc_average)
dend_ward <- as.dendrogram(hc_ward)

# Customize dendrograms (optional: add colors)
dend_complete <- color_branches(dend_complete, k = changepoint_k)
dend_single <- color_branches(dend_single, k = changepoint_k)
dend_average <- color_branches(dend_average, k = changepoint_k)
dend_ward <- color_branches(dend_ward, k = changepoint_k)

par(mfrow = c(1, 2))  # Arrange plots in a 2x2 grid
plot(dend_complete, main = "Complete Linkage", leaflab = "none", ylab = "height", las = 1)
plot(dend_single, main = "Single Linkage",  leaflab = "none", ylab = "height", las = 1)
plot(dend_average, main = "Average Linkage",  leaflab = "none", ylab = "height", las = 1)
plot(dend_ward, main = "Ward's Method",  leaflab = "none", ylab = "height", las = 1)
par(mfrow = c(1, 1))  # Reset plotting layout
```

```{r}
clusters <- cutree(hc_ward, k = changepoint_k)

# Add cluster assignments to the original data
rfm$Cluster <- clusters

ggplot(rfm, aes(x = Recency, y = Frequency, color = Cluster)) +
  geom_point() +
  labs(title = "Recency vs Frequency by Cluster", x = "Recency", y = "Frequency") +
  facet_wrap(~ Cluster) +
  theme_minimal()


ggplot(rfm, aes(x = Recency, y = Monetary, color = Cluster)) +
  geom_point() +
  labs(title = "Recency vs Monetary by Cluster", x = "Recency", y = "Monetary") +
  facet_wrap(~ Cluster) +
  theme_minimal()


ggplot(rfm, aes(x = Frequency, y = Monetary, color = Cluster)) +
  geom_point() +
  labs(title = "Frequency vs Monetary by Cluster", x = "Frequency", y = "Monetary") +
  facet_wrap(~ Cluster) +
  theme_minimal()

cat("Are 1 and 12 in the same cluster:", (rfm$Cluster[rfm$id == 1] == rfm$Cluster[rfm$id==12]))
```

In our hierarchical clustering, we transformed each variable into z-scores to reduce differences in scale. We calculated the distance metric using dist(), which defaults to Euclidean distance. I determined the appropriate number of segments (k = 8) using the elbow method, comparing the height differences of the k values to make my choice. I compared the different dendrograms with k = 8 to see how the clusters were chosen and decided to use the Ward's method to minimize variance within clusters. This resulted in final clusters that were more dissimilar as seen in the dendrogram's final merge height. It is notable that the single linkage version was not able to separate the clusters effectively, chaining all observations together. Using this method, customers 1 and 12 were in the same cluster.
:::

## c. Implement k-means.  (3 pts)

- Describe any pre-processing steps you took (e.g., scaling)
- State the number of segments/clusters you used with justification. 
- Using your segmentation, are customers 1 and 12 in the same cluster?     
    
::: {.callout-note title="Solution"}
```{r}
# Function to compute total within-cluster sum of squares (WSS)
wss <- function(data, k) {
  kmeans(data, centers = k, nstart = 25)$tot.withinss
}

# Compute WSS for 1 to 10 clusters
k_values <- 1:10
wss_values <- sapply(k_values, function(k) wss(rfm_std, k))
elbow_plot = data.frame(WSS = wss_values, Clusters = k_values)

# Function to find the elbow point
find_elbow <- function(wss_values) {
  n <- length(wss_values)
  curvature <- numeric(n)
  for (i in 2:(n - 1)) {
    curvature[i] <- (wss_values[i-1] - wss_values[i]) - (wss_values[i] - wss_values[i+1])
  }
  elbow_point <- which.max(curvature)
  return(elbow_point)
}

# Find the optimal number of clusters
changepoint_k <- find_elbow(wss_values)

# Add a new column to the elbow_plot to color the changepoint
elbow_plot <- elbow_plot %>%
  mutate(color = ifelse(Clusters == changepoint_k, "red", "black"))

# Plot the elbow curve
ggplot(elbow_plot, aes(x = Clusters, y = WSS)) +
  geom_line() +
  geom_point(aes(color = color)) +
  scale_color_identity() +
  labs(title = "Elbow Method for Determining Optimal Clusters",
       x = "Number of Clusters", y = "Total Within-Cluster Sum of Squares")

```
```{r}
# Perform K-means clustering with the chosen number of clusters
set.seed(666)  # Set seed for reproducibility
kmeans_result <- kmeans(rfm_std, centers = changepoint_k, nstart = 25)

# Add the cluster assignments to the data
rfm$Cluster_k <- kmeans_result$cluster

ggplot(rfm, aes(x = Recency, y = Frequency, color = Cluster_k)) +
  geom_point() +
  labs(title = "Recency vs Frequency by Cluster", x = "Recency", y = "Frequency") +
  facet_wrap(~ Cluster_k) +
  theme_minimal()


ggplot(rfm, aes(x = Recency, y = Monetary, color = Cluster_k)) +
  geom_point() +
  labs(title = "Recency vs Monetary by Cluster", x = "Recency", y = "Monetary") +
  facet_wrap(~ Cluster_k) +
  theme_minimal()


ggplot(rfm, aes(x = Frequency, y = Monetary, color = Cluster_k)) +
  geom_point() +
  labs(title = "Frequency vs Monetary by Cluster", x = "Frequency", y = "Monetary") +
  facet_wrap(~ Cluster_k) +
  theme_minimal()

cat("Are 1 and 12 in the same cluster:", (rfm$Cluster_k[rfm$id == 1] == rfm$Cluster_k[rfm$id==12]))
```
Again we used the same z-score scaling. I used a curvature calculation to find the optimal number of clusters. This time, k = 2. Customers 1 and 12 are still in the same cluster by this method.
:::

## d. Implement model-based clustering (3 pts)

- Describe any pre-processing steps you took (e.g., scaling)
- State the number of segments/clusters you used with justification. 
- Describe the best model. What restrictions are on the shape of the components?
- Using your segmentation, are customers 1 and 100 in the same cluster?     

::: {.callout-note title="Solution"}
```{r}
set.seed(666)
cust_clust <- Mclust(data, G = 1:9)
```
```{r}
# Print the summary of the model
summary(cust_clust)

# Extract the optimal number of clusters based on the lowest BIC
optimal_k <- cust_clust$G
cat("The optimal number of clusters is: ", optimal_k)

# Plot the BIC values for different models and number of clusters
plot(cust_clust, what = "BIC", legendArgs=NULL)

title("BIC for Different Numbers of Clusters")

# Classification plot (shows the cluster assignments in the feature space)
plot(cust_clust, what = "classification")

title("Classification for Each Cluster", line = 2.5, adj = 0)

# Uncertainty plot (shows the uncertainty of the cluster assignments)
plot(cust_clust, what = "uncertainty")

title("Uncertainty for each Cluster", line = 2.5, adj = 0)


```
```{r}
rfm$Cluster_mod <- cust_clust$classification
rfm$Cluster_mod <- as.factor(rfm$Cluster_mod)

ggplot(rfm, aes(x = Recency, y = Frequency, color = Cluster_mod)) +
  geom_point() +
  labs(title = "Recency vs Frequency by Cluster", x = "Recency", y = "Frequency") +
  facet_wrap(~ Cluster_mod) +
  theme_minimal()


ggplot(rfm, aes(x = Recency, y = Monetary, color = Cluster_mod)) +
  geom_point() +
  labs(title = "Recency vs Monetary by Cluster", x = "Recency", y = "Monetary") +
  facet_wrap(~ Cluster_mod) +
  theme_minimal()


ggplot(rfm, aes(x = Frequency, y = Monetary, color = Cluster_mod)) +
  geom_point() +
  labs(title = "Frequency vs Monetary by Cluster", x = "Frequency", y = "Monetary") +
  facet_wrap(~ Cluster_mod) +
  theme_minimal()


cat("Are 1 and 100 in the same cluster:", (rfm$Cluster_mod[rfm$id == 1] == rfm$Cluster_mod[rfm$id==100]))
```

In the model based clustering approach we use the same z-score scaled features to build our model. The model determined that 8 was the optimal number of clusters for the data observed based on Bayesian information criterion comparisons between several different model structures and number of clusters. The best model was determined to be the most flexible model, varying in all three metrics, shape (ellipsoid vs spherical), volume, and orientation. This allows for flexible placement to better fit our data. Customers 1 and 100 are also in the same cluster when k=8 in this approach.

:::

## e. Discussion of results (3 pts)

Discuss how you would cluster the customers if you had to do this for your job. Do you think one model would do better than the others? 

::: {.callout-note title="Solution"}
I would use the model based clustering because it allows you flexibility with the results. It could have chosen other types of clustering if it would have suited the data better based on the BIC. It also allows for identification of points which meet a certain criterion for ambiguity due to the probabilistic nature of the classification. With respect to our specific features, the plots of Frequency vs Recency and Recency vs Monetary seem to have well-divided clusters, whereas Monetary vs Frequency is less insightful to any patterns in the data. Regardless, my cursory visual inspection leads me to believe that this approach is appropriate at first glance.
:::


# Problem 2: Unbalanced Data (15 pts)

A researcher is trying to build a predictive model for distinguishing between real and AI generated images. She collected a random sample ($n=10,000$) of tweets/posts that included images. Expert analysts were hired to label the images as real or AI generated. They determined that 1000 were AI generated and 9000 were real. 

She tasked her grad student with building a logistic regression model to predict the probability that a new image is AI generated. After reading on the internet, the grad student became concerned that the data was *unbalanced* and fit the model using a weighted log-loss 
$$
-\sum_{i=1}^n w_i \left[ y_i \log \hat{p}(x_i) + (1-y_i) \log (1-\hat{p}(x_i)) \right]
$$
where $y_i = 1$ if AI generated ($y_i=0$ if real) and $w_i = 1$ if $y_i = 1$ (AI) and $w_i = 1/9$ if $y_i = 0$ (real). This makes $\sum_i w_iy_i = \sum_i w_i(1-y_i) = 1000$. That is the total weight of the AI images equals the total weight of the real images. Note: An similar alternative is to downsample the real images; that is, build a model with 1000 AI and a random sample of 1000 real images. The grad student fits the model using the weights and is able to make predictions $\hat{p}(x)$. 

While the grad student is busy implementing this model, the researcher grabbed another 1000 random tweets/posts with images and had the experts again label them real or AI. Excitedly, the grad student makes predictions on the test data. However, the model doesn't seem to be working well on these new test images. While the AUC appears good, the log-loss and brier scores are really bad.

Hint: By using the weights (or undersampling), the grad student is modifying the base rate (prior class probability).

## a. What is going on? (5 pts)

How can the AUC be strong while the log-loss and brier scores aren't. 

::: {.callout-note title="Solution"}
The AUROC takes into account the model's ability to discriminate between classifications based on relative ranking. if the higher probabilities are being assigned to more true positives than true negatives consistently, this score remains high. It doesn't however account for miscalibrated probability at the population level. Since the log-loss and brier scores are sensitive to miscalibration with respect to class probabilities, these scores suffer. 
:::

## b. What is the remedy? (5 pts)

Specifically, how should the grad student adjust the predictions for the new test images? Use equations and show your work. Hints: the model is outputting $\hat{p}(x) = \widehat{\Pr}(Y=1|X=x)$; consider the log odds and Bayes theorem.

::: {.callout-note title="Solution"}
## Derivation of Posterior Adjustment

We start with the **log-loss function** used in training:

\\[
-\\sum_{i=1}^n w_i \\left[ y_i \\log \\hat{p}(x_i) + (1-y_i) \\log (1-\\hat{p}(x_i)) \\right]
\\]

where:  
- \\( w_i \\) are the weights applied to each observation,  
- \\( y_i \\) is the true label (\\(1\\) for the positive class, \\(0\\) for the negative class),  
- \\( \\hat{p}(x_i) = \\widehat{\\Pr}(Y = 1 \\mid X = x_i) \\) is the predicted probability of the positive class for sample \\(i\\), **based on the sampled priors**.  

The model is trained using undersampled data, where the class proportions (priors) are altered, e.g., to 50-50. As a result, \\( \\hat{p}(x) \\) reflects the posterior probabilities under the **sampled priors**:
\\[
P'(Y = 1) \\quad \\text{and} \\quad P'(Y = 0).
\\]

---

### Bayesian Posterior Adjustment

To adjust the predictions to reflect the true population priors, we use **Bayes' theorem**. The true posterior probability \\( P(Y = 1 \\mid X = x) \\) is:

\\[
P(Y = 1 \\mid X = x) = \\frac{P(X \\mid Y = 1) P(Y = 1)}{P(X)}.
\\]

The denominator \\( P(X) \\) can be expanded as:

\\[
P(X) = P(X \\mid Y = 1) P(Y = 1) + P(X \\mid Y = 0) P(Y = 0).
\\]

Substituting this into the equation gives:

\\[
P(Y = 1 \\mid X = x) = \\frac{P(X \\mid Y = 1) P(Y = 1)}{P(X \\mid Y = 1) P(Y = 1) + P(X \\mid Y = 0) P(Y = 0)}.
\\]

---

### Relating to the Model Output

The model predicts \\( \\hat{p}(x) = \\widehat{\\Pr}(Y = 1 \\mid X = x) \\), which is proportional to \\( P(X \\mid Y = 1) \\) under the **sampled priors**. That is:

\\[
\\hat{p}(x) = \\frac{P(X \\mid Y = 1) P'(Y = 1)}{P(X)}.
\\]

Using this, we can write the true posterior probability in terms of \\( \\hat{p}(x) \\) as:

\\[
P(Y = 1 \\mid X = x) = \\frac{\\hat{p}(x) \\cdot \\frac{P(Y = 1)}{P'(Y = 1)}}{\\hat{p}(x) \\cdot \\frac{P(Y = 1)}{P'(Y = 1)} + (1 - \\hat{p}(x)) \\cdot \\frac{P(Y = 0)}{P'(Y = 0)}}
\\]

Here:  
- \\( P(Y = 1) \\): True prior probability of \\( Y = 1 \\) in the population.  
- \\( P(Y = 0) \\): True prior probability of \\( Y = 0 \\) in the population.  
- \\( P'(Y = 1) \\): Sampled prior probability of \\( Y = 1 \\) (e.g., 0.5 if undersampled to 50-50).  
- \\( P'(Y = 0) \\): Sampled prior probability of \\( Y = 0 \\) (e.g., 0.5 if undersampled to 50-50).  

---

### Final Adjusted Formula

The adjusted posterior probability is:

\\[
P_{\\text{true}}(Y = 1 \\mid X = x) = \\frac{\\widehat{\\Pr}(Y = 1 \\mid X = x) \\cdot \\frac{P(Y = 1)}{P'(Y = 1)}}{\\widehat{\\Pr}(Y = 1 \\mid X = x) \\cdot \\frac{P(Y = 1)}{P'(Y = 1)} + (1 - \\widehat{\\Pr}(Y = 1 \\mid X = x)) \\cdot \\frac{P(Y = 0)}{P'(Y = 0)}}
\\]

This adjustment rescales the probabilities predicted by the model to account for the true population priors, ensuring that the predictions are calibrated to the original class distribution.

:::

## c. Base rate correction (5 pts)

If the grad student's weighted model predicts an image is AI generated with $\hat{p}(x) = .80$, what is the updated prediction under the assumption that the true proportion of AI is 1/10. 

::: {.callout-note title="Solution"}
```{r}
adjust_probabilities <- function(pred_probs, true_priors, sampled_priors) {
  adjusted_probs <- (pred_probs * (true_priors[1] / sampled_priors[1])) /
    (pred_probs * (true_priors[1] / sampled_priors[1]) +
     (1 - pred_probs) * (true_priors[2] / sampled_priors[2]))
  return(adjusted_probs)
}

# Example inputs
p_hat <- 0.8  # Model-predicted probabilities
true_priors <- c(0.1, 0.9)      # True priors (P(Y=1), P(Y=0))
sampled_priors <- c(0.5, 0.5)   # Sampled priors (P'(Y=1), P'(Y=0))

# Adjust the probabilities
adjusted_prob <- adjust_probabilities(p_hat, true_priors, sampled_priors)
cat("The adjusted probability of the weighted model classifying an image AI generated is :", adjusted_prob)

```
As seen above after adjusting for the weighting, the probability of classifying an AI generated image is decreased, though not quite to the observed level in the sample data.
:::


# Problem 3: Multiclass Classification (10 pts)

You have built a predictive model that outputs a probability vector $\hat{p}(x) = [\hat{p}_1(x), \hat{p}_2(x), \hat{p}_3(x)]$ for a 3-class categorical output. 
Consider the following loss matrix which includes an option to return *No Decision* if there is too much uncertainty in the label:

|        | $\hat{G} =1$| $\hat{G} =2$| $\hat{G} =3$| No Decision|
|:-------|------------:|------------:|------------:|-----------:|
|$G = 1$ |            0|            2|            2|           1|
|$G = 2$ |            1|            0|            2|           1|
|$G = 3$ |            1|            1|            0|           1|


What label would you output if the estimated probability is: $\hat{p}(x) = [0.25, 0.15, 0.60]$. Show your work.

::: {.callout-note title="Solution"}
```{r}
p_hat = tibble(                 # initialize vector
  p1 = 0.25, 
  p2 = 0.15,
  p3 = 0.60)

p_hat <- p_hat %>%
  mutate(
    loss_1 = 2 * (p2 + p3),     # Loss for predicting class 1
    loss_2 = p1 + 2 * p3,       # Loss for predicting class 2
    loss_3 = p1 + p2,           # Loss for predicting class 3
    loss_no_decision = 1        # Loss for choosing "No Decision"
  )

p_hat <- p_hat %>%
  mutate(
    decision = case_when(
      loss_1 < pmin(loss_2, loss_3, loss_no_decision) ~ "Class 1",
      loss_2 < pmin(loss_1, loss_3, loss_no_decision) ~ "Class 2",
      loss_3 < pmin(loss_1, loss_2, loss_no_decision) ~ "Class 3",
      TRUE ~ "No Decision" # Default to "No Decision" if no condition is met
    )
  )


cat("The classification for this probability vector is: ", p_hat$decision)

```
:::

# Problem 4: Donor Acceptance Modeling (40 pts)

::: {style="background-color:blue; color:yellow; display: block; border-color: black; padding:1em; padding-bottom: .5em;"}
The data for this problem is for your private use on this exam only. You may not share or use for any other purposes. 
:::

This challenge has you predicting the probability that a pediatric donor heart offer will be Accepted or Rejected. Use the `donor_accept_train.csv` data (available in Canvas) to build a model to predict the probability of `outcome = "Accept"`. The test data `donor_accept_test.csv` is used for making predictions. 

A description of the transplant system and variables is provided in `donor_accept_vars.html`.

Hints: 

- There are four parts to this problem. Before you being think about how your approach will address all four (for example, your choice of model(s) in part a may influence your approach to part c). 

- As always, *before you start coding* write out each step of the process. Think about inputs and outputs. 


## a. Probability Prediction Contest (10 pts)

Build a model to predict the probability that an offer will be accepted. Performance is evaluated using log-loss. 


*Contest Submission:* 

- Submit your predictions on the `donor_accept_test.csv` data. Create a .csv file (ensure comma separated format) named `lastname_firstname.csv` that includes the column named "prob_accept" that is your estimated posterior probability. We will use automated evaluation, so the format must be exact. 


*Notes:*

- I suggest you quickly make an initial model without doing any feature engineering or much tuning. There are a lot of features, an endless number of feature engineering tasks, many predictive models each with many tuning parameters to choose from. But just get something that correctly outputs probabilities and use it to complete the other parts to this problem. You can always come back and improve the model if your time permits. 

- You must show your code. Because your code may take some time to run, you may want to run the model outside this notebook. If you do so, copy the final code into this notebook and set `eval=FALSE` in the corresponding code chunk(s) so we can see the code, but it won't run when the notebook compiles. 


*Competition Grading:*

- 2 of the 10 points are based on readable code
- 3 of the 10 points are based on a valid submission (e.g., correct number of rows and log-loss beats an intercept only model)
- The remaining 5 points are based on your predictive performance. The top score will receive all 5, the second best 4.93, third best 4.85, etc.  


::: {.callout-note title="Solution"}
```{r}
d_train <- read.csv("donor_accept_train-1.csv")
d_test <- read.csv("donor_accept_test.csv")
```
```{r}
library(tidyverse)
library(tidymodels)
library(yardstick)
keep_col = c("OFFER_DATE", "DISTANCE", "OFFER_TOD", "AGE_DON",
             "GENDER_DON", "RACE_DON", "BMI_DON", "ABO_DON",
             "CPR_ADMIN", "CPR_DURATION", "CARDARREST",
             "CARDARREST_DURATION", "COD_DON", "CHEST_TRAUMA",
             "RISK_HIV_DON", "RISK_DRUGS_DON", "RISK_HEP_DON",
             "TATTOOS_DON", "ABNL_ECHO_CUM", "LVSWMA",
             "VALVE_FXN", "GLOBAL_VENT_DYSF", "biplane_eject_frac",
             "qual_eject_frac", "four_chamber_eject_frac",
             "HRS_FROM_DEATH", "T4", "T4_ever", "Dopamine_ever",
             "Epinephrine_ever", "Norepinephrine_ever",
             "Vasopressin_ever", "HGB", "HCT", "GFR_min",
             "PELD_max", "PULSE_MIN", "PULSE_MAX",
             "BP_SYST", "STATUS_CAND", "AGE_CAND", "GENDER_CAND",
             "BMI_CAND", "ECMO_CAND", "LIFE_SUPPORT_CAND",
             "CAND_DIAG_primary", "WEIGHT_RATIO", "AGE_DIFF",
             "DON_HOSP_PREV", "DON_HOSP_TX",
             "LISTING_CTR_PREV_ADVERSE_RATE", "outcome")
d_train <- d_train %>%
  mutate(outcome = factor(outcome, levels = c("Accept", "Reject")),
         OFFER_DATE = as.Date(OFFER_DATE, format = "%Y-%m-%d"))
```
```{r}
recipe <- recipe(outcome ~ ., data = d_train) %>%
  step_select(c("OFFER_DATE", "DISTANCE", "OFFER_TOD", "AGE_DON",
                "BMI_DON", "CARDARREST", "CARDARREST_DURATION",
                "COD_DON", "BMI_CAND", "ECMO_CAND",
                "LIFE_SUPPORT_CAND", "WEIGHT_RATIO", "AGE_DIFF",
             "DON_HOSP_PREV", "DON_HOSP_TX",
             "LISTING_CTR_PREV_ADVERSE_RATE", "outcome"))%>%
  step_mutate(
    day_of_year = as.numeric(format(OFFER_DATE, "%j")),
    offer_sin = sin(2 * pi * day_of_year / 365),
    offer_cos = cos(2 * pi * day_of_year / 365),
    tod_sin = sin(2 * pi * OFFER_TOD / 24),
    tod_cos = cos(2 * pi * OFFER_TOD / 24)
  ) %>%
  step_rm(OFFER_DATE, day_of_year, OFFER_TOD) %>%
  # step_interact(~ CARDARREST:CARDARREST_DURATION,
  #               ECMO_CAND:LIFE_SUPPORT_CAND,
  #               # CPR_ADMIN:CPR_DURATION,
  #               # RISK_DRUGS_DON:RACE_DON,
  #               DON_HOSP_PREV:DON_HOSP_TX) %>%
  step_impute_bag(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())
```
```{r}
library(future)
plan(multisession)
rf_model <- rand_forest(
  mode = "classification",
  mtry = tune(),
  trees = 500,
  min_n = tune()
) %>%
  set_engine("ranger", importance = "permutation")

workflow <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(recipe)

set.seed(666)

rf_resamples <- vfold_cv(d_train, v = 5, strata = outcome)

rf_grid <- grid_regular(
  mtry(range = c(5, 20)),
  min_n(range = c(5,10)),
  levels = 4
)

rf_tune <- tune_grid(
  workflow,
  resamples = rf_resamples,
  grid = rf_grid,
  metrics = metric_set(mn_log_loss)
)

best_rf <- rf_tune %>%
  select_best(metric = "mn_log_loss")
```
```{r}
final_rf <- workflow %>%
  finalize_workflow(best_rf)

rf_fit <- fit(final_rf, data = d_train)

rf_metrics <- rf_fit %>%
  predict(d_test, type = "prob") %>%
  bind_cols(d_test) %>%
  metrics(truth = outcome, .pred_Accept, metric = metric_set(mn_log_loss))

```

:::


## b: Hard Classification (10 pts)

Suppose you are asked to make a hard classification using the probabilities from part a. Making a false negative is 4 times worse that making a false positive (i.e., $C_{FN} = 4*C_{FP}$).

- What threshold should be used with your predictions? How did you choose? 

::: {.callout-note title="Solution"}
```{r}
# Function to calculate the cost at a specific threshold
calculate_cost <- function(predictions, threshold = 0.5) {
  pred_class <- if_else(predictions$.pred_Accept >= threshold, "Accept", "Reject")
  
  # Confusion matrix
  cm <- conf_mat(predictions, truth = outcome, estimate = pred_class)
  
  FP <- cm$table["Reject", "Accept"]  # False Positive
  FN <- cm$table["Accept", "Reject"]  # False Negative
  
  # Cost: FP cost is 1, FN cost is 4
  cost <- FP * 1 + FN * 4
  return(cost)
}

# Try different thresholds and calculate the cost
thresholds <- seq(0, 1, by = 0.01)
costs <- sapply(thresholds, calculate_cost, predictions = predictions)

# Find the threshold that minimizes the cost
optimal_threshold <- thresholds[which.min(costs)]

# Classify using the optimal threshold
predictions <- predictions %>%
  mutate(predicted_class = if_else(.pred_Accept >= optimal_threshold, "Accept", "Reject"))

# Evaluate performance with the new threshold
metrics <- metrics(predictions, truth = outcome, estimate = predicted_class)

# View metrics and optimal threshold
metrics
optimal_threshold

```
:::

- How many of the offers in the test set are classified as *Accept* using this threshold?

:::{.callout-note title="Solution"}
```{r}
# Ensure d_test goes through the same preprocessing steps as d_train using the recipe
preprocessed_test <- bake(recipe, new_data = d_test)

# Apply the same prediction and thresholding process on the preprocessed test set
predictions <- rf_fit %>%
  predict(preprocessed_test, type = "prob") %>%
  bind_cols(preprocessed_test)

# Classify using the threshold and count "Accept" classifications
predictions <- predictions %>%
  mutate(predicted_class = if_else(.pred_Accept >= optimal_threshold, "Accept", "Reject"))

# Count how many offers are classified as "Accept"
num_accepted <- sum(predictions$predicted_class == "Accept")

# Print the result
print(paste("Number of offers classified as Accept: ", num_accepted))
```
:::

## c. Feature Importance (10 pts)

What features are most important? Describe your results and approach in a language that a clinician would want to listen to and can understand. Be clear about the type of feature importance you used, the data (training, testing) that was used to calculate the feature importance scores, and the limitations inherent in your approach to feature importance. 

Notes:

- Your audience is a non-data scientist, so be sure to give a brief high level description of any terms they may not be familiar with. 
- You wouldn't want to show the clinician the feature importance of all 100+ features. Indicate how to select the *most* important features to report. 
- You are not expected to know the clinical meaning of the features. 

:::{.callout-note title="Solution"}
```{r}
library(dplyr)
library(vip)

# Extract feature importance and arrange by importance
importance <- rf_fit %>%
  extract_fit_parsnip() %>%
  vip::vip(method = "model", target = NULL) %>%
  as.data.frame() %>%
  arrange(desc(Importance))  # Arrange by importance

# Select the top 8 predictors
top_8_predictors <- importance %>%
  head(8) %>%
  select(Feature)

# Print the top 8 predictors
print(top_8_predictors)

```
```{r}
# Plot the top 8 predictors
rf_fit %>%
  extract_fit_parsnip() %>%
  vip::vip(method = "permute", target = NULL) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(limits = top_8_predictors$Feature)
```

:::

## d. Calibration (10 pts)

Assess the calibration of your predictions. There are no points off for having a poorly calibrated model; the purpose of this problem is to demonstrate your knowledge of how to evaluate calibration. 

:::{.callout-note title="Solution"}
```{r}
# Create a data frame with predicted probabilities and actual outcomes
calibration_data <- predictions %>%
  mutate(predicted_prob = .pred_Accept,
         actual = if_else(outcome == "Accept", 1, 0)) 

# Bin the predicted probabilities into groups (e.g., 0.0-0.1, 0.1-0.2, etc.)
calibration_binned <- calibration_data %>%
  mutate(prob_bin = cut(predicted_prob, breaks = seq(0, 1, by = 0.1), include.lowest = TRUE)) %>%
  group_by(prob_bin) %>%
  summarize(mean_pred = mean(predicted_prob), 
            observed = mean(actual), 
            n = n()) %>%
  ungroup()

# Plot the calibration curve
ggplot(calibration_binned, aes(x = mean_pred, y = observed)) +
  geom_point() +
  geom_line(aes(x = mean_pred, y = mean_pred), color = "blue", linetype = "dashed") +  # Ideal diagonal
  labs(x = "Predicted Probability", y = "Observed Proportion of 'Accept'", 
       title = "Calibration Plot") +
  theme_minimal()

```
```{r}
# Brier score calculation
brier_score <- calibration_data %>%
  mutate(brier = (predicted_prob - actual)^2) %>%
  summarize(brier_score = mean(brier))

# Print the Brier score
print(paste("Brier Score: ", brier_score$brier_score))

# Calculate log loss for the test set
log_loss_result <- rf_fit %>%
  predict(d_test, type = "prob") %>%
  bind_cols(d_test) %>%
  metrics(truth = outcome, estimate = .pred_Accept, metric = metric_set(log_loss))

# Print the log loss
print(paste("Log-loss: ", log_loss_result))

```
:::

