---
title: "Homework #9: Feature Importance" 
author: "**Ashley Miller**"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
dir_data = 'https://mdporter.github.io/teaching/data/' # data directory
library(tidyverse) # functions for data manipulation
library(tidymodels)
library(ranger)
```


# Problem 1: Permutation Feature Importance 

Vanderbilt Biostats has collected data on Titanic survivors (https://hbiostat.org/data/). I have done some simple processing and split into a training and test sets.

- [titanic_train.csv](`r file.path(dir_data, "titanic_train.csv")`)
- [titanic_test.csv](`r file.path(dir_data, "titanic_test.csv")`)

We are going to use this data to investigate feature importance.
Use `Class`, `Sex`, `Age`, `Fare`, `sibsp` (number of siblings or spouse on board), `parch` (number of parents or children on board), and `Joined` (city where passenger boarded) for the predictor variables (features) and `Survived` as the outcome variable. 

## a. Load the titanic traning and testing data

::: {.callout-note title="Solution"}
```{r}

titanic_train <- read_csv(file.path(dir_data, "titanic_train.csv"))
titanic_test <- read_csv(file.path(dir_data, "titanic_test.csv"))

titanic_train <- titanic_train %>%
  mutate(Survived = as.factor(Survived))
titanic_test <- titanic_test %>%
  mutate(Survived = as.factor(Survived))

```
```{r}
# Count the number of missing values in each column
missing_values <- sapply(titanic_train, function(x) sum(is.na(x)))

# Create a dataframe to view the missing values count
missing_values_df <- data.frame(Feature = names(missing_values),
                                MissingValues = missing_values)

# Sort the missing values in descending order
missing_values_df <- missing_values_df[order(missing_values_df$MissingValues, decreasing = TRUE), ]

print(missing_values_df)

```


:::

## b. Method 1: Built-in importance scores

Fit a tree ensemble model (e.g., Random Forest, boosted tree) on the training data. You are free to use any method to select the tuning parameters.

Report the built-in feature importance scores and produce a barplot with feature on the x-axis and importance on the y-axis. 

::: {.callout-note title="Solution"}
```{r}
head(titanic_train)
summary(titanic_train)
```
```{r}
titanic_recipe <- recipe(Survived ~ Class + Sex + Age + Fare + sibsp + parch + Joined, data = titanic_train) %>%
  step_impute_median(Age, Fare) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

ranger_model <- rand_forest(mtry = tune(), trees = tune()) %>%
  set_engine("ranger", importance = "permutation") %>%
  set_mode("classification")

titanic_workflow <- workflow() %>%
  add_recipe(titanic_recipe) %>%
  add_model(ranger_model)

tune_grid <- grid_regular(mtry(range = c(1, 5)), 
                          trees(range = c(500, 1500)), 
                          levels = 5)

set.seed(666) 
cv_folds <- vfold_cv(titanic_train, v = 5)

tuned_results <- titanic_workflow %>% 
  tune_grid(resamples = cv_folds, grid = tune_grid, 
             control = control_grid(save_pred = TRUE))


best_results <- tuned_results %>% select_best(metric = "accuracy") 

final_workflow <- titanic_workflow %>% finalize_workflow(best_results) 

final_fit <- final_workflow %>% fit(data = titanic_train)
```


```{r}
library(vip)

feature_importance <- vi(final_fit)

vi_df <- as.data.frame(feature_importance)


ggplot(vi_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme_minimal() +
  labs(title = "Variable Importance Plot",
       x = "Feature",
       y = "Importance") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
:::

## c. Performance 

Report the performance of the model fit from (a.) on the test data. Use the log-loss (where $M$ is the size of the test data):
$$ 
\text{log-loss}(\hat{p}) = - \frac{1}{M} \sum_{i=1}^m [y_i \log \, \hat{p}_i + (1 - y_i) \log \, (1 - \hat{p}_i)]
$$

::: {.callout-note title="Solution"}
```{r}
library(yardstick)

titanic_predictions <- final_fit %>%
  predict(new_data = titanic_test, type = "prob") %>%
  bind_cols(titanic_test)

log_loss_result <- titanic_predictions %>%
  mn_log_loss(truth = Survived, .pred_0)

actual <- as.numeric(titanic_predictions$Survived)
predicted <- titanic_predictions$.pred_0


log_loss_manual <- function(actual, predicted, epsilon = 1e-15) { 
  predicted <- pmax(pmin(predicted, 1 - epsilon), epsilon)
  M <- length(actual) 
  log_loss <- - (1 / M) * sum(actual * log(predicted) + (1 - actual)*log(1-predicted))
}
log_loss_result_man <- log_loss_manual(actual, predicted)

print(log_loss_result)

```

:::


## d. Method 2: Permute *after* fitting

Use the fitted model from question (a.) to perform permutation feature importance. Shuffle/permute each variable individually on the *test set* before making predictions. Record the loss. Repeat $M=10$ times and produce a boxplot of the change in loss (change from reported loss from part b.). 

::: {.callout-note title="Solution"}
```{r}

baseline_preds <- predict(final_fit, titanic_test, type = "prob") %>% bind_cols(titanic_test)
baseline_loss <- baseline_preds %>%
  mn_log_loss(truth = Survived, .pred_0) %>%
  pull(.estimate)


set.seed(666)
M <- 10
specified_predictors <- c("Class", "Sex", "Age", "Fare", "sibsp", "parch", "Joined")
feature_importance <- list()

for (col in specified_predictors) {
  losses <- numeric(M)
  
  for (i in 1:M) {
    permuted_test <- titanic_test %>%
      mutate(!!col := sample(!!sym(col)))
    
    permuted_preds <- predict(final_fit, permuted_test, type = "prob") %>%
      bind_cols(permuted_test)
    
    permuted_loss <- permuted_preds %>%
      mn_log_loss(truth = Survived, .pred_0) %>%
      pull(.estimate)
    
    losses[i] <- permuted_loss - baseline_loss
  }
  
  feature_importance[[col]] <- losses
}

vi_df <- do.call(rbind, lapply(names(feature_importance), function(x) {
  data.frame(Variable = x, ChangeInLoss = feature_importance[[x]])
})) %>%
  as_tibble()


ggplot(vi_df, aes(x = reorder(Variable, ChangeInLoss), y = ChangeInLoss)) +
  geom_boxplot() +
  labs(title = "Test Set Permutation Feature Importance",
       x = "Features",
       y = "Change in Loss") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```

:::

## e. Method 3: Permute *before* fitting

For this approach, shuffle/permute the *training data* and re-fit the ensemble model. Evaluate the predictions on the (unaltered) test data. Repeat $M=10$ times (for each predictor variable) and produce a boxplot of the change in loss. 

::: {.callout-note title="Solution"}
```{r}
permute_fit <- function(train_data, test_data, final_workflow, col, M = 10) {
  losses <- numeric(M)
  
  for (i in 1:M) {
    permuted_train <- train_data %>%
      mutate(!!col := sample(!!sym(col)))
    
    permuted_fit <- final_workflow %>% fit(data = permuted_train)
    

    permuted_preds <- predict(permuted_fit, test_data, type = "prob") %>%
      bind_cols(test_data)
    
    permuted_loss <- permuted_preds %>%
      mn_log_loss(truth = Survived, .pred_0) %>%
      pull(.estimate)
    
    losses[i] <- permuted_loss
  }
  
  return(losses)
}

final_fit <- final_workflow %>% fit(data = titanic_train)
baseline_preds <- predict(final_fit, titanic_test, type = "prob") %>% bind_cols(titanic_test)
baseline_loss <- baseline_preds %>%
  mn_log_loss(truth = Survived, .pred_0) %>%
  pull(.estimate)

set.seed(666)
specified_predictors <- c("Class", "Sex", "Age", "Fare", "sibsp", "parch", "Joined")
feature_importance <- list()

for (col in specified_predictors) {
  losses <- permute_fit(titanic_train, titanic_test, final_workflow, col, M = 10)
  feature_importance[[col]] <- losses - baseline_loss
}

vi_df <- do.call(rbind, lapply(names(feature_importance), function(x) {
  data.frame(Variable = x, ChangeInLoss = feature_importance[[x]])
})) %>%
  as_tibble()

ggplot(vi_df, aes(x = reorder(Variable, ChangeInLoss), y = ChangeInLoss)) +
  geom_boxplot() +
  labs(title = "Training Set Permutation Feature Importance Pre-fit",
       x = "Features",
       y = "Change in Loss") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

:::


## f. Understanding 

Describe the benefits of each of the three approaches to measure feature importance. 

::: {.callout-note title="Solution"}
The first method with built in scores is useful when the package has the functionality natively as it is efficient and is interpretable since it is tailored to the model. It doesn't, however, allow us to prescribe the metric as we are limited to the choices made by the package creator, and also doesn't work with models that don't build in importance scores. It also gives us a point estimate as opposed to a set of values. The second method is useful because it is generalizable to any model, and tells us more about how a certain predictor affects model performance. It also isn't computationally intensive like the last method. The third method tells us more about how each predictor affects the model's ability to learn from the training data.
:::

# Problem 2: Effects of correlated predictors

This problem will illustrate what happens to the importance scores when there are highly associated predictors. 

## a. Create an almost duplicate feature

Create a new feature `Sex2` that is 95% the same as `Sex`. Do this by selecting 5% of training ($n=50$) and testing ($n=15$) data and flip the `Sex` value. 

::: {.callout-note title="Solution"}
```{r}
set.seed(666)
n_train <- nrow(titanic_train)
n_flip_train <- ceiling(0.05 * n_train)

titanic_train <- titanic_train %>%
  mutate(Sex2 = Sex) %>%  
  mutate(Sex2 = ifelse(row_number() %in% sample(n_train, n_flip_train), 
                       ifelse(Sex == "male", "female", "male"), 
                       Sex))

set.seed(666)
n_test <- nrow(titanic_test)
n_flip_test <- ceiling(0.05 * n_test)

titanic_test <- titanic_test %>%
  mutate(Sex2 = Sex) %>%  
  mutate(Sex2 = ifelse(row_number() %in% sample(n_test, n_flip_test), 
                       ifelse(Sex == "male", "female", "male"), 
                       Sex))
```

:::

## b. Method 1: Built-in importance

Fit the same model as in Problem 1b, but use the new data that includes `Sex2` (i.e., use both `Sex` and `Sex2` in the model). Calculate the built-in feature importance score and produce a barplot. 

::: {.callout-note title="Solution"}
```{r}
titanic_recipe <- recipe(Survived ~ Class + Sex + Sex2 + Age + Fare + sibsp + parch + Joined, data = titanic_train) %>%
  step_impute_median(Age, Fare) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

ranger_model <- rand_forest(mtry = tune(), trees = tune()) %>%
  set_engine("ranger", importance = "permutation") %>%
  set_mode("classification")

titanic_workflow <- workflow() %>%
  add_recipe(titanic_recipe) %>%
  add_model(ranger_model)

tune_grid <- grid_regular(mtry(range = c(1, 5)), 
                          trees(range = c(500, 1500)), 
                          levels = 5)

set.seed(666) 
cv_folds <- vfold_cv(titanic_train, v = 5)

tuned_results <- titanic_workflow %>% 
  tune_grid(resamples = cv_folds, grid = tune_grid, 
             control = control_grid(save_pred = TRUE))


best_results <- tuned_results %>% select_best(metric = "accuracy") 

final_workflow <- titanic_workflow %>% finalize_workflow(best_results) 

final_fit <- final_workflow %>% fit(data = titanic_train)
```


```{r}


feature_importance <- vi(final_fit)

vi_df <- as.data.frame(feature_importance)


ggplot(vi_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme_minimal() +
  labs(title = "Variable Importance Plot",
       x = "Feature",
       y = "Importance") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```


:::

## c. Method 2: Permute *after* fitting

Redo Method 2 (problem 1d) on the new data/model and produce a boxplot of importance scores. The importance score is defined as the difference in loss.

::: {.callout-note title="Solution"}
```{r}
baseline_preds <- predict(final_fit, titanic_test, type = "prob") %>% bind_cols(titanic_test)
baseline_loss <- baseline_preds %>%
  mn_log_loss(truth = Survived, .pred_0) %>%
  pull(.estimate)


set.seed(666)
M <- 10
specified_predictors <- c("Class", "Sex", "Sex2", "Age", "Fare", "sibsp", "parch", "Joined")
feature_importance <- list()

for (col in specified_predictors) {
  losses <- numeric(M)
  
  for (i in 1:M) {
    permuted_test <- titanic_test %>%
      mutate(!!col := sample(!!sym(col)))
    
    permuted_preds <- predict(final_fit, permuted_test, type = "prob") %>%
      bind_cols(permuted_test)
    
    permuted_loss <- permuted_preds %>%
      mn_log_loss(truth = Survived, .pred_0) %>%
      pull(.estimate)
    
    losses[i] <- permuted_loss - baseline_loss
  }
  
  feature_importance[[col]] <- losses
}

vi_df <- do.call(rbind, lapply(names(feature_importance), function(x) {
  data.frame(Variable = x, ChangeInLoss = feature_importance[[x]])
})) %>%
  as_tibble()


ggplot(vi_df, aes(x = reorder(Variable, ChangeInLoss), y = ChangeInLoss)) +
  geom_boxplot() +
  labs(title = "Test Set Permutation Feature Importance",
       x = "Features",
       y = "Change in Loss") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

:::

## d. Method 3: Permute *before* fitting

Redo Method 3 (problem 1e) on the new data and produce a boxplot of importance scores. The importance score is defined as the difference in loss.

::: {.callout-note title="Solution"}
```{r}
permute_fit <- function(train_data, test_data, final_workflow, col, M = 10) {
  losses <- numeric(M)
  
  for (i in 1:M) {
    permuted_train <- train_data %>%
      mutate(!!col := sample(!!sym(col)))
    
    permuted_fit <- final_workflow %>% fit(data = permuted_train)
    

    permuted_preds <- predict(permuted_fit, test_data, type = "prob") %>%
      bind_cols(test_data)
    
    permuted_loss <- permuted_preds %>%
      mn_log_loss(truth = Survived, .pred_0) %>%
      pull(.estimate)
    
    losses[i] <- permuted_loss
  }
  
  return(losses)
}

final_fit <- final_workflow %>% fit(data = titanic_train)
baseline_preds <- predict(final_fit, titanic_test, type = "prob") %>% bind_cols(titanic_test)
baseline_loss <- baseline_preds %>%
  mn_log_loss(truth = Survived, .pred_0) %>%
  pull(.estimate)

set.seed(666)
specified_predictors <- c("Class", "Sex", "Sex2", "Age", "Fare", "sibsp", "parch", "Joined")
feature_importance <- list()

for (col in specified_predictors) {
  losses <- permute_fit(titanic_train, titanic_test, final_workflow, col, M = 10)
  feature_importance[[col]] <- losses - baseline_loss
}

vi_df <- do.call(rbind, lapply(names(feature_importance), function(x) {
  data.frame(Variable = x, ChangeInLoss = feature_importance[[x]])
})) %>%
  as_tibble()

ggplot(vi_df, aes(x = reorder(Variable, ChangeInLoss), y = ChangeInLoss)) +
  geom_boxplot() +
  labs(title = "Training Set Permutation Feature Importance Pre-fit",
       x = "Features",
       y = "Change in Loss") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

:::

## e. Understanding

Describe how the addition of the almost duplicated predictor impacted the feature importance results.  

::: {.callout-note title="Solution"}
Our built in feature importance did not rank the duplicate noisy predictor nearly as high as the original. Permuting the test set made the original most important variable, Sex, even more important and Sex2 was not impactful in the change in loss. Permuting the training set resulted in a smaller change in loss for the original variable, perhaps because the noisy version picked up some of the slack.
:::

