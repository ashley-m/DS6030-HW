---
title: "Homework #9: Feature Importance" 
author: "**Ashley Miller**"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
dir_data = 'https://mdporter.github.io/teaching/data/' # data directory
library(tidyverse) # functions for data manipulation
library(tidymodels)
```


# Problem 1: Permutation Feature Importance 

Vanderbilt Biostats has collected data on Titanic survivors (https://hbiostat.org/data/). I have done some simple processing and split into a training and test sets.

- [titanic_train.csv](`r file.path(dir_data, "titanic_train.csv")`)
- [titanic_test.csv](`r file.path(dir_data, "titanic_test.csv")`)

We are going to use this data to investigate feature importance.
Use `Class`, `Sex`, `Age`, `Fare`, `sibsp` (number of siblings or spouse on board), `parch` (number of parents or children on board), and `Joined` (city where passenger boarded) for the predictor variables (features) and `Survived` as the outcome variable. 

## a. Load the titanic traning and testing data

::: {.callout-note title="Solution"}
```{r}

titanic_train <- read_csv(file.path(dir_data, "titanic_train.csv"))
titanic_test <- read_csv(file.path(dir_data, "titanic_test.csv"))

head(titanic_train)
head(titanic_test)



table(titanic_train$Job)
```
```{r}
# Count the number of missing values in each column
missing_values <- sapply(titanic_train, function(x) sum(is.na(x)))

# Create a dataframe to view the missing values count
missing_values_df <- data.frame(Feature = names(missing_values),
                                MissingValues = missing_values)

# Sort the missing values in descending order
missing_values_df <- missing_values_df[order(missing_values_df$MissingValues, decreasing = TRUE), ]

print(missing_values_df)

```
```{r}
titanic_train <- titanic_train[, !names(titanic_train) %in% c("Job", "Occupation")]
titanic_train <- titanic_train %>%
  mutate(
    Fare = ifelse(is.na(Fare), mean(Fare, na.rm = TRUE), Fare),
    Age = ifelse(is.na(Age), mean(Age, na.rm = TRUE), Age),
    Class = as.factor(Class),
    Name_ID = as.factor(Name_ID)
  )

titanic_test <- titanic_test[, !names(titanic_test) %in% c("Job", "Occupation")]
titanic_test <- titanic_test %>%
  mutate(
    Fare = ifelse(is.na(Fare), mean(Fare, na.rm = TRUE), Fare),
    Age = ifelse(is.na(Age), mean(Age, na.rm = TRUE), Age),
    Class = as.factor(Class),
    Name_ID = as.factor(Name_ID)
  )

```

:::

## b. Method 1: Built-in importance scores

Fit a tree ensemble model (e.g., Random Forest, boosted tree) on the training data. You are free to use any method to select the tuning parameters.

Report the built-in feature importance scores and produce a barplot with feature on the x-axis and importance on the y-axis. 

::: {.callout-note title="Solution"}
```{r}
head(titanic_train)
summary(titanic_train)
```

```{r}
library(xgboost)
library(caret)

# Data preparation
features <- titanic_train[, -which(names(titanic_train) %in% c("Survived"))]
labels <- titanic_train$Survived

dummies <- dummyVars(~ ., data = features)
one_hot_data <- predict(dummies, newdata = features)

data_matrix <- as.matrix(one_hot_data)
label_vector <- as.numeric(labels)

# xgboost parameters
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = 0.1,
  max_depth = 3,
  nthread = parallel::detectCores()
)

dtrain <- xgb.DMatrix(data = data_matrix, label = label_vector)

set.seed(666)
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  verbose = 1
)

```


```{r}

# Get feature importance scores
importance_scores <- xgb.importance(model = xgb_model)

# Rename 'Gain' to 'Importance'
importance_df <- importance_scores %>%
  rename(Importance = Gain)

# Plot feature importance
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Feature Importance in XGBoost Model",
       x = "Feature",
       y = "Importance") +
  theme_minimal()

```
```{r}
library(ranger)

set.seed(666)

# Train the model
rf_model <- ranger(Survived ~ Class + Sex + Age + Fare + sibsp + parch + Joined,
                   data = titanic_train, num.trees = 100,
                   importance = 'impurity')

```
```{r}
# Get importance scores
importance_scores <- rf_model$variable.importance

# Create a dataframe to view the feature importance
importance_df <- data.frame(Feature = names(importance_scores),
                            Importance = importance_scores)

# Sort the importance scores in descending order
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

print(importance_df)
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Feature Importance in XGBoost Model",
       x = "Feature",
       y = "Importance") +
  theme_minimal()
```


:::

## c. Performance 

Report the performance of the model fit from (a.) on the test data. Use the log-loss (where $M$ is the size of the test data):
$$ 
\text{log-loss}(\hat{p}) = - \frac{1}{M} \sum_{i=1}^m [y_i \log \, \hat{p}_i + (1 - y_i) \log \, (1 - \hat{p}_i)]
$$

::: {.callout-note title="Solution"}
```{r}
comb = rbind(features_train, features_test)
# Prepare the training data
features_train <- titanic_train[, -which(names(titanic_train) %in% c("Survived", "Job", "Occupation"))]
labels_train <- titanic_train$Survived

dummies <- dummyVars(~ ., data = comb)
one_hot_train <- predict(dummies, newdata = features_train)

# Prepare the test data
features_test <- titanic_test[, -which(names(titanic_test) %in% c("Survived", "Job", "Occupation"))]
one_hot_test <- predict(dummies, newdata = features_test)

# Convert to matrices
train_matrix <- as.matrix(one_hot_train)
test_matrix <- as.matrix(one_hot_test)

# Align columns by adding missing columns with zeros in the test set
missing_cols <- setdiff(colnames(train_matrix), colnames(test_matrix))
for (col in missing_cols) {
  test_matrix <- cbind(test_matrix, matrix(0, nrow = nrow(test_matrix), ncol = 1)) 
  colnames(test_matrix)[ncol(test_matrix)] <- col
}
# Ensure columns are in the same order
test_matrix <- test_matrix[, colnames(train_matrix)]

# Convert labels to numeric
label_train_vector <- as.numeric(labels_train)
label_test_vector <- as.numeric(titanic_test$Survived)

# Create DMatrix for training and test data
dtrain <- xgb.DMatrix(data = train_matrix, label = label_train_vector)
dtest <- xgb.DMatrix(data = test_matrix, label = label_test_vector)

# Set xgboost parameters
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = 0.1,
  max_depth = 3,
  nthread = parallel::detectCores()
)

# Train the xgboost model
set.seed(666)
xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,
  verbose = 1
)

# Generate predictions for the test set
predicted_probs <- predict(xgb_model, dtest)

# Compute log-loss for the test set
log_loss <- -mean(label_test_vector * log(predicted_probs) + (1 - label_test_vector) * log(1 - predicted_probs))

# Print log-loss
print(log_loss)

```

:::


## d. Method 2: Permute *after* fitting

Use the fitted model from question (a.) to perform permutation feature importance. Shuffle/permute each variable individually on the *test set* before making predictions. Record the loss. Repeat $M=10$ times and produce a boxplot of the change in loss (change from reported loss from part b.). 

::: {.callout-note title="Solution"}
Add solution here
:::

## e. Method 3: Permute *before* fitting

For this approach, shuffle/permute the *training data* and re-fit the ensemble model. Evaluate the predictions on the (unaltered) test data. Repeat $M=10$ times (for each predictor variable) and produce a boxplot of the change in loss. 

::: {.callout-note title="Solution"}
Add solution here
:::


## f. Understanding 

Describe the benefits of each of the three approaches to measure feature importance. 

::: {.callout-note title="Solution"}
Add solution here
:::

# Problem 2: Effects of correlated predictors

This problem will illustrate what happens to the importance scores when there are highly associated predictors. 

## a. Create an almost duplicate feature

Create a new feature `Sex2` that is 95% the same as `Sex`. Do this by selecting 5% of training ($n=50$) and testing ($n=15$) data and flip the `Sex` value. 

::: {.callout-note title="Solution"}
Add solution here
:::

## b. Method 1: Built-in importance

Fit the same model as in Problem 1b, but use the new data that includes `Sex2` (i.e., use both `Sex` and `Sex2` in the model). Calculate the built-in feature importance score and produce a barplot. 

::: {.callout-note title="Solution"}
Add solution here
:::

## c. Method 2: Permute *after* fitting

Redo Method 2 (problem 1d) on the new data/model and produce a boxplot of importance scores. The importance score is defined as the difference in loss.

::: {.callout-note title="Solution"}
Add solution here
:::

## d. Method 3: Permute *before* fitting

Redo Method 3 (problem 1e) on the new data and produce a boxplot of importance scores. The importance score is defined as the difference in loss.

::: {.callout-note title="Solution"}
Add solution here
:::

## e. Understanding

Describe how the addition of the almost duplicated predictor impacted the feature importance results.  

::: {.callout-note title="Solution"}
Add solution here
:::

