---
title: "Homework #9: Feature Importance" 
author: "**Ashley Miller**"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
dir_data = 'https://mdporter.github.io/teaching/data/' # data directory
library(tidyverse) # functions for data manipulation
library(tidymodels)
```


# Problem 1: Permutation Feature Importance 

Vanderbilt Biostats has collected data on Titanic survivors (https://hbiostat.org/data/). I have done some simple processing and split into a training and test sets.

- [titanic_train.csv](`r file.path(dir_data, "titanic_train.csv")`)
- [titanic_test.csv](`r file.path(dir_data, "titanic_test.csv")`)

We are going to use this data to investigate feature importance.
Use `Class`, `Sex`, `Age`, `Fare`, `sibsp` (number of siblings or spouse on board), `parch` (number of parents or children on board), and `Joined` (city where passenger boarded) for the predictor variables (features) and `Survived` as the outcome variable. 

## a. Load the titanic traning and testing data

::: {.callout-note title="Solution"}
```{r}

titanic_train <- read_csv(file.path(dir_data, "titanic_train.csv"))
titanic_test <- read_csv(file.path(dir_data, "titanic_test.csv"))

head(titanic_train)
head(titanic_test)

titanic_train <- titanic_train %>%
  mutate(across(where(is.character), as.factor),
         Survived = as.factor(Survived))

titanic_test <- titanic_test %>%
  mutate(across(where(is.character), as.factor),
         Survived = as.factor(Survived))

table(titanic_train$Job)
```
```{r}
# Count the number of missing values in each column
missing_values <- sapply(titanic_train, function(x) sum(is.na(x)))

# Create a dataframe to view the missing values count
missing_values_df <- data.frame(Feature = names(missing_values),
                                MissingValues = missing_values)

# Sort the missing values in descending order
missing_values_df <- missing_values_df[order(missing_values_df$MissingValues, decreasing = TRUE), ]

print(missing_values_df)

```
```{r}
titanic_train <- titanic_train[, !names(titanic_train) %in% c("Job", "Occupation")]
titanic_train <- titanic_train %>%
  mutate(
    Fare = ifelse(is.na(Fare), mean(Fare, na.rm = TRUE), Fare),
    Age = ifelse(is.na(Age), mean(Age, na.rm = TRUE), Age),
    Class = as.factor(Class),
    Name_ID = as.factor(Name_ID)
  )

```

:::

## b. Method 1: Built-in importance scores

Fit a tree ensemble model (e.g., Random Forest, boosted tree) on the training data. You are free to use any method to select the tuning parameters.

Report the built-in feature importance scores and produce a barplot with feature on the x-axis and importance on the y-axis. 

::: {.callout-note title="Solution"}
```{r}
head(titanic_train)
summary(titanic_train)
```

```{r}
library(adabag)
library(foreach)
library(doParallel)

# Detect the number of available cores
num_cores <- detectCores()

# Set up parallel backend
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Function to combine models
combine_models <- function(x, y) {
  x$trees <- c(x$trees, y$trees)
  x$weights <- c(x$weights, y$weights)
  x
}

# Define control parameters for decision trees
control_params <- rpart.control(maxdepth = 1)

# Fit AdaBoost model in parallel
set.seed(666)
ada_models <- foreach(i = 1:16, .packages = 'adabag') %dopar% {
  boosting(Survived ~ Class + Sex + Age + Fare + sibsp + parch + Joined, 
           data = titanic_train, 
           boos = TRUE, 
           mfinal = 13, 
           control = control_params)
}

# Combine models sequentially
ada_model <- Reduce(combine_models, ada_models)

stopCluster(cl)
registerDoSEQ()

# Display the combined model
print(ada_model)

```
```{r}
control_params <- rpart.control(maxdepth = 1)
ada_model = boosting(Survived ~ Class + Sex + Age + Fare + sibsp + parch + Joined, 
           data = titanic_train, 
           boos = TRUE, 
           mfinal = 13, 
           control = control_params)
```


```{r}

# Get feature importance scores
importance_scores <- ada_model$importance

# Convert to a data frame for plotting
importance_df <- as.data.frame(importance_scores)
importance_df$Feature <- rownames(importance_df)
importance_df <- importance_df %>%
  rownames_to_column(var = "Feature") %>%
  rename(Importance = V1)

# Plot feature importance
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Feature Importance in AdaBoost Model",
       x = "Feature",
       y = "Importance") +
  theme_minimal()

```
```{r}
library(ranger)

set.seed(666)

# Train the model
rf_model <- ranger(Survived ~ Class + Sex + Age + Fare + sibsp + parch + Joined,
                   data = titanic_train, num.trees = 100,
                   importance = 'impurity')

```
```{r}
# Get importance scores
importance_scores <- rf_model$variable.importance

# Create a dataframe to view the feature importance
importance_df <- data.frame(Feature = names(importance_scores),
                            Importance = importance_scores)

# Sort the importance scores in descending order
importance_df <- importance_df[order(importance_df$Importance, decreasing = TRUE), ]

print(importance_df)
```


:::

## c. Performance 

Report the performance of the model fit from (a.) on the test data. Use the log-loss (where $M$ is the size of the test data):
$$ 
\text{log-loss}(\hat{p}) = - \frac{1}{M} \sum_{i=1}^m [y_i \log \, \hat{p}_i + (1 - y_i) \log \, (1 - \hat{p}_i)]
$$

::: {.callout-note title="Solution"}
Add solution here
:::


## d. Method 2: Permute *after* fitting

Use the fitted model from question (a.) to perform permutation feature importance. Shuffle/permute each variable individually on the *test set* before making predictions. Record the loss. Repeat $M=10$ times and produce a boxplot of the change in loss (change from reported loss from part b.). 

::: {.callout-note title="Solution"}
Add solution here
:::

## e. Method 3: Permute *before* fitting

For this approach, shuffle/permute the *training data* and re-fit the ensemble model. Evaluate the predictions on the (unaltered) test data. Repeat $M=10$ times (for each predictor variable) and produce a boxplot of the change in loss. 

::: {.callout-note title="Solution"}
Add solution here
:::


## f. Understanding 

Describe the benefits of each of the three approaches to measure feature importance. 

::: {.callout-note title="Solution"}
Add solution here
:::

# Problem 2: Effects of correlated predictors

This problem will illustrate what happens to the importance scores when there are highly associated predictors. 

## a. Create an almost duplicate feature

Create a new feature `Sex2` that is 95% the same as `Sex`. Do this by selecting 5% of training ($n=50$) and testing ($n=15$) data and flip the `Sex` value. 

::: {.callout-note title="Solution"}
Add solution here
:::

## b. Method 1: Built-in importance

Fit the same model as in Problem 1b, but use the new data that includes `Sex2` (i.e., use both `Sex` and `Sex2` in the model). Calculate the built-in feature importance score and produce a barplot. 

::: {.callout-note title="Solution"}
Add solution here
:::

## c. Method 2: Permute *after* fitting

Redo Method 2 (problem 1d) on the new data/model and produce a boxplot of importance scores. The importance score is defined as the difference in loss.

::: {.callout-note title="Solution"}
Add solution here
:::

## d. Method 3: Permute *before* fitting

Redo Method 3 (problem 1e) on the new data and produce a boxplot of importance scores. The importance score is defined as the difference in loss.

::: {.callout-note title="Solution"}
Add solution here
:::

## e. Understanding

Describe how the addition of the almost duplicated predictor impacted the feature importance results.  

::: {.callout-note title="Solution"}
Add solution here
:::

